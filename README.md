# ğŸ” LinkedIn-Scraper

This project uses `browser-use` â€” an LLM-powered browser automation library â€” to scrape LinkedIn connections and extract structured profile data such as names and URLs. It combines browser control, language models, memory handling, and persistent sessions to collect data from dynamically rendered web pages.

---

## âš™ï¸ Technical Specs

### ğŸ§  High-Level Design

- **Agent-Based Automation**: Uses a custom agent (`ContextManagedAgent`) built on `browser-use` to navigate LinkedIn and interact with dynamic content.
- **LLM-Powered**: Integrates Google Gemini via `langchain_google_genai` for reasoning and deciding browser actions.
- **Memory Management**: Implements a `ConversationTokenBufferMemory` to limit context to 3000 tokens.
- **DOM Filtering**: Truncates DOM to focus on interactive elements (like `button`, `a`, and `input`) for LLM efficiency.
- **Caching**: Employs `PersistentBrowser` to retain browser sessions for faster repeat runs and bypass login flow.
- **Context Resetting**: Reloads the page and resets memory every 5 steps to avoid model confusion due to stale context.
- **Loop Prevention**: Limits agent steps and tracks identical actions to avoid infinite loops.

### ğŸ“š Libraries Used

- `browser-use`
- `langchain`
- `langchain-google-genai`
- `pydantic`
- `python-dotenv`
- `asyncio`, `re`, `json`, `os`

---

## ğŸš€ Setup & Usage

### 1ï¸âƒ£ Clone the Repo

```bash
git clone https://github.com/yourusername/LinkedIn-Scraper.git
cd LinkedIn-Scraper
```

### 2ï¸âƒ£ Setup Python Environment using `uv`

Make sure you have [uv](https://github.com/astral-sh/uv) installed.

```bash
uv venv --python 3.11
```

#### Activate the Environment

```bash
# For Mac/Linux:
source .venv/bin/activate

# For Windows:
.venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
uv pip install -r requirements.txt
```

### 4ï¸âƒ£ Add Your Credentials

Create a `.env` file in the project root with the following:

```env
LINKEDIN_EMAIL=your_email@example.com
LINKEDIN_PASSWORD=your_password
GEMINI_API_KEY=your_google_generative_ai_key
```

### 5ï¸âƒ£ Run the Scraper

```bash
python agent.py
```

### ğŸ§ª Example Output

After running the script, you'll get a `linkedin_connections.json` file like:

```json
[
  {
    "name": "John Doe",
    "profile_url": "https://www.linkedin.com/in/john-doe/"
  },
  {
    "name": "Jane Smith",
    "profile_url": "https://www.linkedin.com/in/jane-smith/"
  }
]
```

---

## ğŸ’¡ Discussion

### ğŸ§± Challenges Faced

- **LinkedIn Anti-Scraping**: LinkedIn uses bot-detection techniques and requires login. Solved this by using `PersistentBrowser` sessions and performing login only once.
- **Dynamic Content Loading**: LinkedIn loads data asynchronously, so we added `wait_for_network_idle_page_load_time` to ensure full page load before DOM interaction.
- **Context Limitations**: LLMs have limited memory. We truncated the DOM to the most relevant interactive elements and reset memory every 5 steps.
- **Loop Prevention**: Set max iteration safeguards and tracked redundant actions to detect when the agent gets stuck.
- **Headless vs Headed Mode**: During debugging, headed mode (`highlight_elements=True`) helped visualize the agentâ€™s behavior.

### âœ… Optimizations Implemented

- Used `ConversationTokenBufferMemory` to retain relevant info within the token budget.
- Filtered irrelevant DOM nodes to reduce context bloat.
- Cached browser sessions to reduce login frequency.
- Reloaded stale pages and context periodically.

---

## ğŸ“‚ Output & Caching

- Scraped data is stored in `linkedin_connections.json`.
- Session logs are saved in the `logs/conversation/` folder.
- Browser state is cached using `PersistentBrowser` under the profile `linkedin_scraper`.

---

## ğŸ™‹â€â™‚ï¸ Contributions

Feel free to open issues or submit pull requests for improving scraping logic, optimizing memory usage, or adding other platforms.

---

## ğŸ“„ License

This project is licensed under the MIT License.

```

Let me know if you'd like this as a downloadable file or want me to auto-generate it in your repo structure!
```
